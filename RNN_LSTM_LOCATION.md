# ğŸ”„ RNN Model Location & Explanation

## Where is the RNN Model?

### âœ… RNN Model = LSTM (Objective 2)

**RNN (Recurrent Neural Network)** is the general category.
**LSTM (Long Short-Term Memory)** is a specific type of RNN that we implemented.

---

## ğŸ“ File Locations

### 1. LSTM Implementation
**File:** `models/lstm_forecasting.py`

This contains:
- `AccidentLSTM` class - The RNN/LSTM model
- Training functions
- Prediction functions
- Data preprocessing for time-series

### 2. Trained Model
**File:** `models/lstm_forecaster.pth`

This is the saved trained model (PyTorch checkpoint).

### 3. Dashboard Page
**File:** `pages/8_ğŸ“…_Forecasting.py`

This is where users can:
- Train new LSTM models
- Load existing models
- Make 7-day forecasts
- Visualize predictions

---

## ğŸ§  What is LSTM/RNN?

### RNN vs LSTM

```
RNN (Recurrent Neural Network)
â”œâ”€â”€ Simple RNN (basic, has vanishing gradient problem)
â”œâ”€â”€ LSTM (Long Short-Term Memory) â† What you implemented
â””â”€â”€ GRU (Gated Recurrent Unit)
```

### Why LSTM for Your Project?

**Problem:** Predict how many accidents will happen next week

**Why RNN/LSTM?**
- Accidents have **temporal patterns** (time-based)
- Monday â‰  Saturday (different patterns)
- Summer â‰  Winter (seasonal trends)
- Need to remember **past patterns** to predict future

**Traditional ML can't do this:**
```
Random Forest: Predicts one accident at a time
âŒ Can't predict "How many accidents tomorrow?"

LSTM: Looks at past 30 days â†’ Predicts next day
âœ… Can forecast future accident counts
```

---

## ğŸ—ï¸ LSTM Architecture (Your Implementation)

### Data Transformation

```python
# Original Data (Transactional)
Row 1: Accident on 2024-01-15 at 14:30
Row 2: Accident on 2024-01-15 at 16:45
Row 3: Accident on 2024-01-16 at 09:20
...

# Transformed to Time-Series
Date         | Accident Count
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2024-01-15   | 245
2024-01-16   | 198
2024-01-17   | 223
2024-01-18   | 267
...

# Create Sequences (30-day windows)
Input: [245, 198, 223, 267, ..., 189]  (30 days)
Output: 201                              (day 31)
```

### Model Architecture

```
Input: 30 days of accident counts
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LSTM Layer 1 (64 units)       â”‚  â† Learns short-term patterns
â”‚   - Remembers recent trends     â”‚
â”‚   - Dropout 0.2                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LSTM Layer 2 (64 units)       â”‚  â† Learns long-term patterns
â”‚   - Remembers weekly cycles     â”‚
â”‚   - Dropout 0.2                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fully Connected Layer         â”‚  â† Makes final prediction
â”‚   64 â†’ 1                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Output: Predicted accident count for tomorrow
```

### How LSTM "Remembers"

```python
# LSTM has 3 gates (like memory management)

1. Forget Gate: "What should I forget?"
   - Forgets old patterns that don't matter anymore
   - Example: Forget accident spike from 3 weeks ago

2. Input Gate: "What should I remember?"
   - Remembers important new information
   - Example: Remember that weekends have fewer accidents

3. Output Gate: "What should I output?"
   - Decides what to use for prediction
   - Example: Use weekly pattern + recent trend
```

---

## ğŸ“Š LSTM vs TabTransformer

| Aspect | LSTM (RNN) | TabTransformer |
|--------|------------|----------------|
| **Purpose** | Time-series forecasting | Classification |
| **Input** | Sequence of numbers | Single record |
| **Output** | Future value | Category |
| **Task** | "How many accidents tomorrow?" | "What type of collision?" |
| **Data Type** | Temporal (time-based) | Tabular (features) |
| **Memory** | Sequential memory | Attention mechanism |

### Example Tasks

**LSTM (Time-Series):**
```python
# Input: Past 30 days
[245, 198, 223, 267, 189, 234, ...]

# Output: Next day prediction
Predicted: 212 accidents tomorrow
```

**TabTransformer (Classification):**
```python
# Input: Single accident features
{lum: 3, agg: 1, int: 2, hour: 18, ...}

# Output: Collision type
Predicted: "Side collision" (51% confidence)
```

---

## ğŸ¯ Your LSTM Implementation Details

### Configuration
```python
AccidentLSTM(
    input_size=1,        # 1 feature (accident count)
    hidden_size=64,      # 64 LSTM units
    num_layers=2,        # 2 LSTM layers
    dropout=0.2,         # 20% dropout
    sequence_length=30   # Look at past 30 days
)
```

### Training
```python
Epochs: 100
Batch Size: 32
Optimizer: Adam (learning_rate=0.001)
Loss: MSE (Mean Squared Error)
Device: CPU
```

### Data Flow
```
Historical Data (2005-2024)
  â†“
Aggregate by day â†’ Daily counts
  â†“
Create sequences â†’ [30 days] â†’ [next day]
  â†“
Normalize â†’ Scale to [0, 1]
  â†“
Train LSTM â†’ Learn patterns
  â†“
Predict â†’ Forecast next 7 days
  â†“
Denormalize â†’ Convert back to actual counts
```

---

## ğŸš€ How to Use LSTM in Dashboard

### Step 1: Navigate to Forecasting Page
```
Dashboard â†’ ğŸ“… Forecasting
```

### Step 2: Train or Load Model
```python
# Option A: Train new model
Click "Train New Model"
- Loads historical data
- Trains for 100 epochs
- Saves to models/lstm_forecaster.pth

# Option B: Load existing model
Automatically loads if lstm_forecaster.pth exists
```

### Step 3: Generate Forecast
```python
# Model predicts next 7 days
Today: 245 accidents
Tomorrow: 212 accidents (predicted)
Day 2: 198 accidents (predicted)
Day 3: 223 accidents (predicted)
...
Day 7: 189 accidents (predicted)
```

### Step 4: View Visualization
```
Chart shows:
- Historical data (blue line)
- Predictions (red line)
- Confidence intervals (shaded area)
```

---

## ğŸ”¬ Technical Comparison

### All Three Models Side-by-Side

| Model | Type | Input | Output | Use Case |
|-------|------|-------|--------|----------|
| **Random Forest** | Tree Ensemble | Features | Collision Type | Classification |
| **XGBoost** | Gradient Boosting | Features | Collision Type | Classification |
| **TabTransformer** | Transformer | Features | Collision Type | Classification |
| **LSTM** | RNN | Time Sequence | Future Count | Forecasting |

### Different Problems, Different Models

```
Problem 1: "What type of collision will this be?"
â†’ Use: TabTransformer / XGBoost / Random Forest
â†’ Input: [lum=3, agg=1, hour=18, ...]
â†’ Output: "Side collision"

Problem 2: "How many accidents next week?"
â†’ Use: LSTM (RNN)
â†’ Input: [245, 198, 223, 267, ...] (past 30 days)
â†’ Output: [212, 198, 223, ...] (next 7 days)
```

---

## ğŸ“ˆ LSTM Performance

### What It Learns

1. **Weekly Patterns**
   - Monday-Friday: More accidents (work commute)
   - Saturday-Sunday: Fewer accidents

2. **Seasonal Trends**
   - Summer: More accidents (more travel)
   - Winter: Fewer accidents (less travel)

3. **Holiday Effects**
   - Before holidays: Spike in accidents
   - During holidays: Drop in accidents

4. **Long-term Trends**
   - Overall increase/decrease over years
   - Policy changes impact

### Evaluation Metrics
```python
MSE (Mean Squared Error): How far off predictions are
MAE (Mean Absolute Error): Average prediction error
RÂ² Score: How well model fits data (0-1, higher is better)
```

---

## ğŸ“ Key Differences Summary

### LSTM (RNN) - Time-Series Forecasting
```
Purpose: Predict FUTURE values
Input: Sequence of past values
Memory: Sequential (remembers order)
Output: Continuous number
Example: "212 accidents tomorrow"
```

### TabTransformer - Classification
```
Purpose: Classify CURRENT record
Input: Single record with features
Memory: Attention (learns relationships)
Output: Category label
Example: "Side collision"
```

### Both Use Deep Learning
```
LSTM: Recurrent connections (loops)
TabTransformer: Attention mechanism (no loops)

Both: Neural networks trained with backpropagation
```

---

## ğŸ“ Quick Reference

### Files to Check

1. **LSTM Implementation**
   ```
   models/lstm_forecasting.py
   - AccidentLSTM class (lines 20-80)
   - Training function (lines 100-200)
   - Prediction function (lines 250-300)
   ```

2. **LSTM Dashboard**
   ```
   pages/8_ğŸ“…_Forecasting.py
   - Training interface
   - Prediction visualization
   - Model loading
   ```

3. **Trained Model**
   ```
   models/lstm_forecaster.pth
   - Saved weights
   - Model configuration
   - Training history
   ```

---

## ğŸ¯ Summary

**RNN Model Location:**
- âœ… Implementation: `models/lstm_forecasting.py`
- âœ… Trained Model: `models/lstm_forecaster.pth`
- âœ… Dashboard: `pages/8_ğŸ“…_Forecasting.py`

**What It Does:**
- Predicts future accident counts (time-series forecasting)
- Uses past 30 days to predict next 7 days
- Learns weekly and seasonal patterns

**Difference from TabTransformer:**
- LSTM: Sequential data â†’ Future prediction
- TabTransformer: Tabular data â†’ Classification

Both are deep learning, but solve different problems! ğŸš€
